{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This package\n",
    "from multiagent_wrapper import Multiagent_wrapper\n",
    "from multiagent_critic import Multiagent_critic\n",
    "from m3ddpg import M3DDPG\n",
    "\n",
    "#The environment\n",
    "import laserhockey.hockey_env as h_env\n",
    "\n",
    "#Things used to make implementations for the interface\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "#just used for the sake of clarity here\n",
    "from typing import List, Union, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing the Multiagent_wrapper interface for the specific environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiagent_laserhockey_wrapper(Multiagent_wrapper):\n",
    "    def __init__(self):\n",
    "        #MANDETORY: gather all neccesarriy information to initialize the super class:\n",
    "        env = h_env.HockeyEnv()\n",
    "        state_space = env.observation_space\n",
    "        num_agents = 2\n",
    "        action_spaces = [gym.spaces.Box(-1.0, 1.0, [4], np.float32)]*2\n",
    "        observation_spaces = [env.observation_space]*2\n",
    "        super().__init__(env, state_space, num_agents, action_spaces, observation_spaces)\n",
    "\n",
    "        #OPTIONAL: specific scaling factor for this environment\n",
    "        self.scaling_vector = np.array([1.0, 1.0, 0.5, 4.0, 4.0, 4.0, 1.0, 1.0, 0.5, 4.0, 4.0, 4.0, 2.0, 2.0, 10.0, 10.0, 4.0, 4.0])\n",
    "\n",
    "    def _build_joint_action(self, actions: List[np.array]) -> np.array:\n",
    "        #MANDETORY: This function combines the actions such that they can be passed into the specific gym environment\n",
    "        return np.hstack(actions)\n",
    "\n",
    "    def _build_observations(self, state: np.array) -> List[np.array]:\n",
    "        #MANDETORY: This function splits the state of the specific environment into observations for each agent and returns them in a list.\n",
    "        return [state/self.scaling_vector, self.env.obs_agent_two()/self.scaling_vector]\n",
    "\n",
    "    def _build_rewards(self, state: np.array, reward: float, info: Union[None, Dict]) -> List[float]:\n",
    "        #MANDETORY: This function calculates the rewards for each agent and returns them in a list.\n",
    "        pure_reward_p1 = reward - info[\"reward_closeness_to_puck\"]\n",
    "        reward_p1 = max(-1., pure_reward_p1)\n",
    "        reward_p2 = max(-1., -pure_reward_p1)\n",
    "        return [reward_p1, reward_p2]\n",
    "\n",
    "    def _build_state(self, state: np.array) -> np.array:\n",
    "        #OPTIONAL: Can be used to preprocess the state (used by the critics). Here it is used for scaling.\n",
    "        return state/self.scaling_vector\n",
    "    \n",
    "#initialising environment\n",
    "env = Multiagent_laserhockey_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating actor Module, usual torch.nn.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HockeyActorNet(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, min_value, max_value):\n",
    "        super(HockeyActorNet, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(  \n",
    "            nn.Linear(in_dim,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, out_dim)\n",
    "        )\n",
    "\n",
    "        self.register_buffer('min_value', torch.tensor(min_value, requires_grad=False, dtype=torch.float32))\n",
    "        self.register_buffer('max_value', torch.tensor(max_value, requires_grad=False, dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.clip(self.layers(x), self.min_value, self.max_value)\n",
    "\n",
    "#initialising actors\n",
    "actor1 =  HockeyActorNet(in_dim=env.observation_spaces[0].shape[0], out_dim=env.action_spaces[0].shape[0], min_value=min(env.action_spaces[0].low), max_value=max(env.action_spaces[0].high))\n",
    "actor2 =  HockeyActorNet(in_dim=env.observation_spaces[1].shape[0], out_dim=env.action_spaces[1].shape[0], min_value=min(env.action_spaces[1].low), max_value=max(env.action_spaces[1].high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Critic module inheriting the Multiagent_critic interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HockeyCriticNet(Multiagent_critic):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.tensor, actions: List[torch.tensor]) -> torch.tensor:\n",
    "        #MANDETORY: This function describes how to process the state and the list \n",
    "        combined = torch.hstack([state,*actions])\n",
    "        return self.layers(combined)\n",
    "    \n",
    "#initialising critics\n",
    "critic1 = HockeyCriticNet(in_dim=env.state_space.shape[0]+env.action_space.shape[0])\n",
    "critic2 = HockeyCriticNet(in_dim=env.state_space.shape[0]+env.action_space.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional, creating burn in policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking given policies from the environment, adding noise and mixing them for better exploration effect.\n",
    "strong_opponent = h_env.BasicOpponent(weak=False)\n",
    "weak_opponent = h_env.BasicOpponent(weak=True)\n",
    "\n",
    "def _add_noise_to_action(action, noise_level, noise_clip, action_low, action_high):\n",
    "    action = torch.tensor(action, dtype=torch.float32, requires_grad=False)\n",
    "    action_high = torch.tensor(action_high, dtype=torch.float32, requires_grad=False)\n",
    "    action_low =torch.tensor(action_low, dtype=torch.float32, requires_grad=False)\n",
    "    noise = (torch.randn_like(action) * noise_level).clip(-noise_clip, noise_clip)\n",
    "    return torch.max(torch.min(action + noise, action_high), action_low).numpy()\n",
    "\n",
    "burnin_policies = [\n",
    "    lambda obs: _add_noise_to_action(np.random.choice([strong_opponent,weak_opponent]).act(env.env._get_obs()),0.2, 1., env.action_spaces[0].low, env.action_spaces[0].high),\n",
    "    lambda obs: _add_noise_to_action(np.random.choice([strong_opponent,weak_opponent]).act(env.env._get_obs()),0.2, 1., env.action_spaces[1].low, env.action_spaces[1].high)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choose hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and pass previously initialised parameters, see Docstring of m3ddpg class for short description of parameters\n",
    "m3ddpg = M3DDPG(env= env, \n",
    "            actor_models = [actor1, actor2],\n",
    "            critic_models = [critic1, critic2],\n",
    "            actor_learning_rates = [0.001, 0.001],\n",
    "            critic_learning_rates = [0.001, 0.001],\n",
    "            device = \"cpu\",\n",
    "            discounts = [0.99, 0.99],\n",
    "            taus = [0.005, 0.005],\n",
    "            noise_levels = [0.2, 0.2],\n",
    "            critic_noise_levels = [0.02, 0.02],\n",
    "            noise_clips = [0.5, 0.5],\n",
    "            epsilons = [0.2, 0.2],\n",
    "            alphas = [1., 1.],\n",
    "            batch_size=64,\n",
    "            burnin_steps=100000,\n",
    "            burnin_policies=burnin_policies,\n",
    "            max_replay_buffer_size = 1000000,\n",
    "            update_target_nets_frequency = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 100100/100100 [01:49<00:00, 914.25it/s]\n"
     ]
    }
   ],
   "source": [
    "rewards = m3ddpg.train(num_train_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the actual load_status function just takes a directory and lists of file names \n",
    "#this is just a way to creat those filename lists\n",
    "\n",
    "#creating file names\n",
    "PREFIX = \"M3DDPG\"\n",
    "DIR_PATH = \"./models/\"\n",
    "NUM_ACTORS = 2\n",
    "ITERATIONS = 4000000\n",
    "\n",
    "actor_file_names, critic_file_names, actor_optimizer_file_names, critic_optimizer_file_names = [], [], [], []\n",
    "for i in range(NUM_ACTORS):\n",
    "    actor_file_names.append(f'{PREFIX}_actor{i}_{ITERATIONS}its.pt')\n",
    "    critic_file_names.append(f'{PREFIX}_critic{i}_{ITERATIONS}its.pt')\n",
    "    actor_optimizer_file_names.append(f'{PREFIX}_actor{i}_optimizer_{ITERATIONS}its.pt')\n",
    "    critic_optimizer_file_names.append(f'{PREFIX}_critic{i}_optimizer_{ITERATIONS}its.pt')\n",
    "    \n",
    "#loading models and optimizers\n",
    "m3ddpg.load_status(dir_path=DIR_PATH, \n",
    "                   actor_file_names=actor_file_names,\n",
    "                  critic_file_names=critic_file_names,\n",
    "                  actor_optimizer_file_names=actor_optimizer_file_names,\n",
    "                  critic_optimizer_file_names=critic_optimizer_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = [m3ddpg.get_policy(i) for i in range(NUM_ACTORS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observer policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    state, obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = []\n",
    "        for i in range(NUM_ACTORS):\n",
    "            actions.append(actors[i](obs[i]))\n",
    "        state, obs, rerwards, done, info = env.step(actions)\n",
    "        env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
